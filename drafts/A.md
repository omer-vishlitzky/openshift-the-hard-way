The Architecture of Determinism: A Comprehensive Analysis of the OpenShift Assisted Installer1. Executive Summary: Inverting the Provisioning ParadigmThe deployment of enterprise Kubernetes on bare metal infrastructure has historically presented a formidable engineering challenge, often referred to in systems architecture as the "Layer 2 Gap." Traditional cloud-native installation methods, such as Installer Provisioned Infrastructure (IPI), predicate their success on the existence of a programmable API that can instantiate, configure, and network virtual machines on demand. In the bare metal domain, such APIs are frequently absent, fragmented across vendor-specific Baseboard Management Controllers (BMCs), or restricted by rigid network security policies. Conversely, User Provisioned Infrastructure (UPI) shifts the burden of integration entirely to the administrator, necessitating complex, fragile external automation to bridge the gap between a blank server and a functional operating system.The OpenShift Assisted Installer (AI) represents a fundamental architectural inversion of these established models. It abandons the traditional "push-based" paradigm—where a central installer attempts to reach out and manipulate infrastructure—in favor of a "pull-based" agent architecture. In this model, the infrastructure itself is responsible for reaching inward to a central control plane to receive its identity, configuration, and operational directives. This shift transforms the provisioning process from a brittle sequence of imperative scripts into a robust, declarative state machine operation.This report provides an exhaustive, forensic-level analysis of the Assisted Installer ecosystem. It deconstructs the installation process not merely as a sequence of administrative steps, but as a complex interplay of distributed state machines, ephemeral runtimes, and immutable infrastructure principles. The analysis traverses the complete lifecycle of a node: from the generation of a specialized discovery ISO to the complex "pivot" mechanism that allows a bootstrap node to transmute into a production control plane member without re-provisioning. By examining the specific mechanics of the "Rendezvous Host," the "Bootstrap-in-Place" methodology, and the precise transitions of the host state machine, this document serves as a definitive reference for engineers seeking to understand the internal causality of the OpenShift installation process on bare metal.2. Architectural Foundations and Component TaxonomyTo comprehend the operational flow of the Assisted Installer, one must first establish a rigorous taxonomy of its constituent components. The system is not a monolithic binary but a distributed microservices architecture that spans the central management plane and the edge devices (the bare metal hosts).2.1 The Control Plane: Assisted Service (assisted-service)The assisted-service functions as the central nervous system of the installation ecosystem. It acts as the authoritative source of truth for the state of all clusters, hosts, and infrastructure environments under management.Architectural Role and Deployment Models:The service is designed as a standard REST API server, typically written in Go, backed by a relational database (PostgreSQL) for state persistence and an object store (S3 or a local filesystem abstraction) for artifact storage. It supports multiple deployment topologies:SaaS Model: Running as a multi-tenant service on the Red Hat Hybrid Cloud Console.On-Premise Hub: Deployed as a pod within an existing management cluster (e.g., via Red Hat Advanced Cluster Management or the Multicluster Engine operator).Standalone/Disconnected: Running as a container on a bastion host or embedded within the Agent-Based Installer (ABI) for air-gapped environments.Core Responsibilities:State Machine Management: The service implements a Finite State Machine (FSM) for every host and cluster resource. It enforces rigid transition rules (e.g., a host cannot transition to installing if it is not in known state with passing validations).Dynamic Ignition Generation: Unlike UPI, where administrators manually craft Ignition configuration files, the assisted-service dynamically generates these manifests. It merges static templates with dynamic data derived from the database (user-supplied networking, proxy settings, SSH keys) at the moment of request.Pre-flight Validation Engine: The service executes a complex rules engine that evaluates the collective inventory of a cluster against OpenShift's requirements. This includes checks for CPU architecture compatibility, RAM thresholds, disk I/O performance, and network overlap detection.2.2 The Edge Actor: The Discovery Agent (assisted-installer-agent)The assisted-installer-agent is the "scout" of the system. It is a lightweight Go binary packaged into the Discovery ISO. Its lifecycle is tied to the ephemeral "Live ISO" environment; it runs only during the discovery and pre-installation phases.Operational Mechanics:Introspection: Upon startup, the agent utilizes hardware inspection libraries (such as ghw and standard Linux syscalls) to traverse the /sys and /proc filesystems. It constructs a comprehensive JSON inventory of the host's physical attributes, including CPU flags, memory DIMM details, storage devices (size, type, model, WWN), and network interfaces (MAC addresses, speed, link state).Connectivity Verification: The agent performs active network probing. It receives instructions from the service to attempt Layer 2 (ARP/Ping) and Layer 3 connections to other agents in the same cluster, effectively verifying the mesh network topology required for etcd and API communication before installation begins.Command Polling: The agent operates on a polling loop, sending heartbeats to the assisted-service API and executing returned "Step" instructions.2.3 The Worker: The Installer (assisted-installer)It is critical to distinguish the agent from the installer. The assisted-installer is a container image that is pulled and executed only when the host transitions to the installing state.Operational Mechanics:Privileged Execution: This container runs with privileged security context, as it requires direct write access to the host's block devices (e.g., /dev/sda).CoreOS Integration: It acts as a wrapper around the coreos-installer binary, translating the high-level directives from the assisted-service into specific command-line arguments that drive the operating system provisioning.Progress Reporting: It parses the standard output of the underlying installation tools to report granular progress (e.g., "Writing image: 45%") back to the control plane.2.4 The Artifact Generator: Assisted Image Service (assisted-image-service)The assisted-image-service is a specialized microservice focused solely on the generation of bootable media.The Latency Problem and the Injection Solution:Generating a full 1GB+ ISO image from scratch for every user request is computationally prohibitive and creates unacceptable latency. To solve this, the image service employs an Ignition Injection technique.Base Image Caching: The service maintains a cache of the generic "RHCOS Live ISO." This image contains the standard kernel, initramfs, and root filesystem.Ignition Embedding: When a user requests an ISO, the service generates a minimal Ignition config containing the specific identity of the request (the infra-env-id) and the URL of the assisted-service.Header Manipulation: Using the coreos-installer iso ignition embed functionality (or library equivalent), the service injects this small JSON payload into the reserved header area of the cached ISO.Result: This process takes milliseconds rather than minutes, producing a unique ISO that boots a generic OS but immediately knows "who it is" and "who to call."3. The Discovery Phase: From Nothing to "Known"The journey of a bare metal host begins in a state of anonymity. We will trace the technical execution flow of how a machine moves from a powered-off piece of metal to a registered, validated candidate for a Kubernetes cluster.3.1 The Boot Sequence: Loading the RAM DiskThe process initiates when the physical machine boots from the generated Discovery ISO (via USB, Virtual Media, or PXE).Bootloader (ISOLINUX/GRUB): The bootloader initiates the kernel. It passes critical Kernel Arguments (kargs) defined in the ISO.coreos.live.rootfs_url: Points the kernel to the location of the root filesystem image (usually embedded in the ISO or fetched via HTTP in PXE scenarios).ignition.firstboot: Instructs the system to run Ignition.Initramfs and Ignition (First Pass): The Red Hat Enterprise Linux CoreOS (RHCOS) kernel loads the initramfs. The Ignition binary executes in the initramfs stage. It reads the embedded Ignition config (injected by the assisted-image-service).Filesystem Creation: Ignition writes the initial configuration files to the RAM disk (since no hard drive is mounted yet)./etc/assisted/agent.yaml: Contains the SERVICE_BASE_URL, CLUSTER_ID (if applicable), INFRA_ENV_ID, and authentication tokens./etc/systemd/system/agent.service: The systemd unit definition for the discovery agent.3.2 Agent Startup and RegistrationThe systemd init process starts the agent.service. This service uses podman to pull (if not present) and run the assisted-installer-agent container.The Registration Handshake:The agent reads the agent.yaml configuration and constructs its first HTTP request.Method: POST /api/assisted-install/v2/infra-envs/{id}/hostsPayload: Architecture (x86_64, s390x, etc.), basic boot info.Service Logic: The assisted-service receives this request. The HostStateMachine triggers the RegisterHost event.Transition: [Empty] → Discovering.Implication: A record is created in the hosts table of the database. The UI updates to show a new host with status "Discovering."3.3 The Introspection LoopOnce registered, the agent enters a persistent polling loop. The default interval is typically 60 seconds, though this can vary based on the stage. The agent sends a heartbeat, and the Service responds with a Step instruction.Instruction: get_inventoryThe Service instructs the agent to perform a full hardware audit.Execution: The agent executes binaries like lshw, lsblk, ip addr, and nmcli.Data Collection:Disks: It filters block devices to identify valid installation targets (excluding USBs, Read-Only media). It checks for WWNs to ensure uniqueness.Memory: It verifies total usable RAM.CPU: It counts cores and verifies virtualization flags (vmx/svm).Submission: The inventory is serialized to JSON and POSTed back to the Service.Instruction: connectivity_check
The Service instructs the agent to verify network paths.Payload: The instruction contains a list of IP addresses and MAC addresses of other agents currently registered in the cluster.Execution: The agent uses ping, arping, and potentially nmap to test reachability.Insight: This effectively maps the physical network topology. The Service can determine if all hosts are on the same Layer 2 broadcast domain (required for VIP failover) or if they are routed (requiring different configurations).3.4 Validation and State convergenceThe assisted-service runs its validation engine against the collected data.Hardware Validation: Does the host meet the minimum requirements (e.g., 16GB RAM, 4 vCPUs)?Network Validation: Is the hostname resolvable? Are there duplicate IP addresses?Time Synchronization: The agent checks NTP synchronization status.State Transition:If all blocking validations pass, the Host State transitions:Discovering → Known (or KnownUnbound if not assigned to a cluster).Status: "Ready" (if bound to a cluster and validations pass) or "Insufficient" (if requirements are unmet).4. The Pre-Installation Phase: The Architecture of ConfigThe transition from a "Ready" cluster to an "Installing" cluster is a critical boundary. Once the installation begins, the assisted-service freezes the configuration and generates the immutable artifacts required for deployment.4.1 The Infrastructure Environment (InfraEnv)The InfraEnv is a crucial abstraction in the Assisted Installer. It represents the logical container for the physical hosts. It defines the environmental constraints under which the hosts operate.Proxy Configuration: If the environment requires an HTTP proxy, it is defined here and injected into the Agent's configuration.SSH Keys: The public keys authorized to access the nodes are defined here.Network Configuration (NMState): This is where the user defines static IP addressing. The Assisted Installer utilizes the Kubernetes NMState operator's syntax to define declarative network states.4.2 Ignition Generation: The Master BlueprintOne of the most complex responsibilities of the assisted-service is the dynamic generation of Ignition configuration files. Unlike traditional UPI where a single master.ign is shared by all masters, the Assisted Installer generates a unique Ignition file for every single host.The Composition of an Ignition File:The final Ignition file served to a host is a merger of several distinct layers:Base CoreOS Config: Standard users (core), systemd units for the kubelet, and CRI-O configuration.Network Config (NMState): If the user defined static IPs in the InfraEnv, the Service translates these YAML definitions into NetworkManager keyfiles (INI format) and injects them into /etc/NetworkManager/system-connections/ via Ignition.Mechanism: The code iterates through the NMState definitions and creates a storage.files entry in the Ignition JSON for each interface.Controller Config: A systemd unit to start the assisted-installer-controller pod post-installation.Metadata: Files like /etc/hostname and /etc/mco/kubelet.conf that define the node's specific identity.4.3 The Rendezvous Host ElectionIn a High Availability (HA) cluster, the bootstrapping process requires a leader. In standard IPI, this is a temporary external VM. In the Assisted Installer, one of the physical control plane nodes is elected to perform this duty.The Election Algorithm:Selection: The assisted-service logic selects a node to be the "Rendezvous Host" (often referred to as Node 0). Preference is usually given to the node with the lowest ID or explicit user assignment.Configuration Divergence:The Rendezvous Host: Receives a "Bootstrap" Ignition config. This config includes the bootkube binary, the cluster manifests (etcd, api-server, controller-manager), and the scripts to start the temporary control plane.The Follower Masters: Receive a "Master" Ignition config. This config directs them to contact the Rendezvous Host (acting as the Machine Config Server) to fetch their full configuration.5. The Installation Execution: Writing to MetalThe user triggers the installation via the API or UI. The Cluster State moves to Installing. The Agents poll the service and receive the install command.5.1 The assisted-installer ContainerThe Agent pulls the assisted-installer image (e.g., quay.io/openshift/assisted-installer:latest). It executes this container with privileged flags (--privileged, --pid=host, -v /dev:/dev) because it requires unfettered access to the host's hardware.The CoreOS Installer Invocation:
The container executes a wrapper script that constructs the coreos-installer command. Based on research snippets , the command structure typically resembles:Bashcoreos-installer install /dev/sda \
  --ignition-url "https://api.openshift.com/api/assisted-install/v2/clusters/{cid}/hosts/{hid}/ignition" \
  --insecure-ignition \
  --append-karg "ip=192.168.1.10::..." \
  --copy-network \
  --preserve-on-error
Deconstructing the Flags:/dev/sda: The target disk identified during the inventory phase.--ignition-url: The installer fetches the unique Ignition file generated in 4.2. This URL is authenticated using a token passed to the installer environment.--copy-network: Critical Flag. It instructs CoreOS to copy the active network configuration (from the RAM disk environment) to the target disk. This ensures that if the node required complex static IPs or VLANs to reach the internet during discovery, those settings persist after reboot.--append-karg: Injects kernel arguments for the first boot, often setting the ip argument to ensure network connectivity before NetworkManager fully starts.5.2 The Disk Partitioning and WriteThe coreos-installer performs the destructive actions:Wipe: It clears the partition table of the target device.Partitioning: It creates the standard RHCOS partition layout:bios-boot / efi-system: For the bootloader.boot: For the kernel and initramfs.root: A 3GB+ partition for the OS image (OSTree).var: A scalable partition for container data and logs.Image Write: It writes the OS image (OSTree commit) to the root partition.Ignition Write: It writes the fetched Ignition configuration to a reserved area (often the boot partition or a dedicated ignition partition depending on the version).Progress Reporting:The assisted-installer wrapper parses the stdout of the coreos-installer and calculates a percentage. It sends HostProgress events to the Service."Writing image: 20%...""Writing image: 90%..."5.3 The Reboot and HandoffUpon successful write, the assisted-installer sends a final status update (Installed) to the Service. The Service transitions the Host State to Installed (pending reboot). The installer then executes systemctl reboot.The node shuts down. The Live ISO environment is lost. The system boots from the hard disk for the first time.6. The Bootstrap and The Pivot: A Deep DiveThis section addresses the user's specific request for "pivot, bootstrap, all that." It is the most technically intricate phase of the deployment, involving the transmutation of a temporary bootstrap node into a permanent control plane member.6.1 The Boot Sequence (Node 0 - Rendezvous Host)When the Rendezvous Host boots from the hard disk:Ignition (Second Pass): Ignition runs in the initramfs. It detects the config written to disk. It applies the configuration: creating users, writing files, and enabling systemd units.Network Start: NetworkManager starts, using the connection profiles copied by the --copy-network flag.bootkube.service Start: Because this node received the "Bootstrap" Ignition, it has the bootkube.service enabled.6.2 The bootkube Processbootkube is the engine of cluster creation. It is responsible for rendering the initial Kubernetes manifests and starting the temporary control plane.The Static Pod Lifecycle:Kubernetes nodes run a kubelet. The kubelet can be configured to watch a local directory (usually /etc/kubernetes/manifests) for Pod definitions. These are "Static Pods"—they are not managed by the API server; they are the API server (and other components).bootkube places the following manifests into the static pod directory:etcd-bootstrap: A single-node etcd cluster.kube-apiserver-bootstrap: The API server.kube-controller-manager-bootstrap: The controller manager.kube-scheduler-bootstrap: The scheduler.machine-config-server-bootstrap: The server that serves Ignition to other nodes.The Cluster Formation:Bootstrap Control Plane Up: The static pods launch. Node 0 now hosts a functional (but temporary) Kubernetes API on port 6443.Follower Nodes Join: The other two control plane nodes boot. Their Ignition config tells them to contact the Machine Config Server (MCS) on Node 0 (via the internal load balancer or DNS VIP).Etcd Expansion: The follower nodes join the etcd cluster. The cluster expands from the single bootstrap etcd to a 3-member cluster.6.3 The Pivot MechanismAt this stage, we have a 3-node etcd cluster. However, Node 0 is still running the "Bootstrap" configuration. It is effectively "dirty" with temporary artifacts and static pods. It must be converted to a standard master node.The Script: bootstrap-pivot.sh
This script is embedded in the bootstrap Ignition and runs as a systemd unit, typically named release-image-pivot.service.Step-by-Step Execution:Wait for Etcd Quorum: The script queries the etcd API. It waits until all 3 members are reported as healthy and the cluster has quorum. This is a critical blocking step. If the network is flaky, the install hangs here.Wait for API Stability: It checks if the Cluster Version Operator (CVO) has started and is processing the release payload.Stop Temporary Services: It stops the bootkube.service. It deletes the static pod manifests from /etc/kubernetes/manifests. This kills the temporary API server on Node 0.Note: The API is maintained by the other two nodes during this brief outage on Node 0.Config Swap (The Pivot):The script identifies the current machine-config corresponding to the "Master" role (managed by the Machine Config Operator).It effectively "re-images" the configuration of the node. It downloads the "Master" Ignition/Configuration from the cluster (which is now self-hosting).It overwrites the local configuration files (users, systemd units, storage configs) with the "Master" version.Restart Kubelet: It restarts the kubelet.service.Rejoin: The kubelet comes back up. It sees the standard "Master" static pods (which are managed by the MCO, not bootkube). It joins the cluster as a regular node.The "Bootstrap-in-Place" (SNO) Variation:
In Single Node OpenShift (SNO), there are no other nodes to hand off to. The pivot process is modified to be "Bootstrap-in-Place".The logic must be extremely careful not to kill the API server before the new configuration is ready to take over.It effectively performs an in-place upgrade of the configuration, ensuring the etcd data directory is preserved across the transition.7. Post-Installation and Day 2 OperationsThe cluster is now technically running, but the installation process is not complete until the assisted-service receives confirmation.7.1 The assisted-installer-controllerThe Ignition config for all master nodes includes a deployment for the assisted-installer-controller pod. This pod runs on the newly formed cluster.Responsibilities:Status Reporting: It connects back to the assisted-service API. It reports "Cluster is accessible."Log Forwarding: It streams the journal logs from the nodes to the Service. This allows users to debug "Day 2" issues via the Assisted Console.CSR Approver: When new nodes (workers) join the cluster, they generate Certificate Signing Requests (CSRs). The standard OCP behavior requires manual approval (or a separate auto-approver). The assisted-installer-controller detects these CSRs and automatically approves them if they match the inventory of the InfraEnv.Console URL Extraction: It queries the OpenShift Route API to find the URL of the Web Console (console-openshift-console.apps...). It pushes this URL and the kubeadmin password back to the Service DB, updating the UI.7.2 The Completion StateWhen the Service receives the final "Success" signal from the controller:Cluster State: Installing → Installed.Host States: Installing → Installed.UI Indication: The user sees the green checkmark and the "Download Kubeconfig" button becomes active.8. Comparative Analysis: AI vs. The AlternativesTo fully appreciate the Assisted Installer's architecture, we must contrast it with the other methodologies mentioned in the query.FeatureIPI (Installer Provisioned)UPI (User Provisioned)Assisted Installer (AI)Agent-Based Installer (ABI)Control PlaneTerraform running on BastionManual or Ansible scriptsAssisted Service (Central)Embedded Service (Local)DiscoveryNone (Assumes cloud API works)None (Assumes user setup)Agent-based IntrospectionAgent-based IntrospectionIgnition DeliveryInjected via Cloud UserDataServed via HTTP ServerFetched via Authenticated APIEmbedded or FetchedBootstrapExternal Ephemeral VM (created/destroyed)External Machine/VMRendezvous Host (Pivot)Rendezvous Host (Pivot)NetworkingDHCP (typically required)Manual Static / DHCPNMState (Integrated/GUI)NMState (Integrated)ValidationFail-fast during installTrial and errorPre-flight Validation EnginePre-flight Validation Engine8.1 The Agent-Based Installer (ABI) NuanceThe Agent-Based Installer  is essentially a packaged version of the Assisted Installer for air-gapped use.Mechanism: When you run openshift-install agent create image, it builds an ISO.Embedded Service: This ISO contains the assisted-service binary itself. When the Rendezvous Host boots, it starts a local copy of the API server.Discovery: The other nodes discover this local service (via Zeroconf/mDNS or static IP configuration) and register with it, exactly as they would with the SaaS version.Handoff: Once the install starts, the local assisted-service shuts down, and the bootkube process takes over the same node.9. Conclusion: The Industrialization of MetalThe OpenShift Assisted Installer represents the maturation of bare metal Kubernetes provisioning. By moving the intelligence from the administrator's workstation to the nodes themselves (via the Agent), and by managing the entire lifecycle through a rigorous State Machine, it eliminates the fragility inherent in script-based installs.Summary of the "Under the Hood" Flow:Define: User defines the desired state (InfraEnv, ClusterDeployment).Generate: assisted-image-service creates an ISO with an embedded identity.Discover: assisted-installer-agent boots, introspects hardware, and proves network viability.Plan: assisted-service generates unique, immutable Ignition plans for each node.Execute: assisted-installer (coreos-installer) commits the OS and Ignition to disk.Bootstrap: Node 0 (Rendezvous) spins up a temporary control plane.Pivot: Node 0 sheds its temporary skin to become a permanent master.Converge: The assisted-installer-controller confirms the cluster is healthy and operational.This architecture ensures that if a cluster can be installed (valid hardware, valid network), it will be installed deterministically, solving the "it works on my laptop" problem of previous bare metal installers.10. Appendix: Data Structures and States10.1 Key Host States (Database Enums)StateDescriptionTriggerDiscoveringAgent has contacted the service. Inventory not yet complete.Agent HTTP RegistrationKnownInventory received. All blocking validations pass.Validation Engine SuccessInsufficientInventory received. Hardware fails requirements.Validation Engine FailurePreparingForInstallationUser clicked install. Artifacts are being generated.API InstallCluster callInstallingAgent has fetched the install command.Polling LoopInstalledcoreos-installer finished successfully.Agent Progress ReportErrorInstallation failed (disk error, network timeout).Agent Error Report10.2 Important Systemd Unitsagent.service: Runs the discovery agent (Live ISO).installer.service: (Ephemeral) Runs the assisted-installer container.bootkube.service: (Bootstrap Node) Runs the temporary control plane.release-image-pivot.service: (Bootstrap Node) Executes the pivot logic.node-image-finish.service: Finalizes OS tree operations (SELinux labeling).machine-config-daemon-firstboot.service: Runs on all nodes during first boot to apply initial MachineConfig.
